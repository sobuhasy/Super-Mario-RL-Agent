{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Training to play Mario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.3)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\sobuh\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.48.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensordict==0.2.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from tensordict==0.2.0) (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from tensordict==0.2.0) (1.26.3)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from tensordict==0.2.0) (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->tensordict==0.2.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->tensordict==0.2.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->tensordict==0.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->tensordict==0.2.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->tensordict==0.2.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from jinja2->torch->tensordict==0.2.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from sympy->torch->tensordict==0.2.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchrl==0.2.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torchrl==0.2.0) (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torchrl==0.2.0) (1.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torchrl==0.2.0) (23.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torchrl==0.2.0) (3.0.0)\n",
      "Requirement already satisfied: tensordict>=0.2.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torchrl==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->torchrl==0.2.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->torchrl==0.2.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->torchrl==0.2.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->torchrl==0.2.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from torch->torchrl==0.2.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from jinja2->torch->torchrl==0.2.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sobuh\\anaconda3\\lib\\site-packages (from sympy->torch->torchrl==0.2.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym-super-mario-bros==7.4.0\n",
    "%pip install tensordict==0.2.0\n",
    "%pip install torchrl==0.2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of RL\n",
    "\n",
    "**Environment**: The world that an agent interacts and learns from.\n",
    "\n",
    "**Action** $a$: How our agent responds to the environment. A set of possible actions can be called action-space.\n",
    "\n",
    "**State** $s$: The current characteristic of the environment. This is a set of all possible states the environment can be in is called state-space.\n",
    "\n",
    "**Reward** $r$: Reward is the key feedback from Environment to agent. It is what drives our agent to learn and change its future action. An aggreation of rewards over multiple time steps is called **Return**.\n",
    "\n",
    "**Action value function** $Q^*(s,a)$: This models the expected return starting from a given state and the agent making a particular action. Given a starting state $s$ takes an arbitrary function $a$, and then for each future time step take the action that maximises returns. The $Q$ stands for the quality of an action in a state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "### Initialisation of the environment\n",
    "\n",
    "In Mario, the environment consists of tubes, mushrooms, enemies and other components.\n",
    "\n",
    "When Mario makes an action, the environment responds with the changed (next) state, reward and other info.\n",
    "\n",
    "Here is the Super Mario environment initialised (in v0.26 we change the render_mode to 'rgb' to see the results on the screen):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sobuh\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\sobuh\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='rgb', apply_api_compatibility=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limits the action space to:\n",
    "\n",
    "0. walk right\n",
    "1. jump right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3), 0.0, False, False, {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sobuh\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = JoypadSpace(env, [['right'], ['right', 'A']])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape}, {reward}, {done}, {trunc}, {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the environment\n",
    "\n",
    "Environment data is returned to the agent in `next_state`. As above, each state is represented by a `[3, 240, 256]` size array. Often that is more information than our agent needs, for instance Mario doesn't response on the color of the pipes or the sky!\n",
    "\n",
    "We use **Wrappers** to preprocess environment data before sending to an another agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SkipFrame` is used to modify the behavior of an OpenAI Gym environment. The `gym.Wrapper` class is a base class provided by the OpenAI Gym library that allows me to add functionality to an environment by wrapping it with additional behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GrayScaleObservation` is a common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information. Now the size of each state: `[1, 240, 256]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "    \n",
    "    def permute_orientation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResizeObservation` downstamps each observation into a square image. The new size is `[1, 84, 84]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "        \n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "        def observation(self, observation):\n",
    "            transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n",
    "            observation = transforms(observation).squeeze(0)\n",
    "            return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Wrappers to the environment\n",
    "`SkipFrame`  is a custom wrapper that inherits from `gym.Wrapper` and implements the `step` function. Because consecutive frames donâ€™t vary much, we can skip n-intermediate frames without losing much information. The n-th frame aggregates rewards accumulated over each skipped frame.\n",
    "\n",
    "`FrameStack` is a wrapper that allows us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can identify if Mario was landing or jumping based on the direction of his movement in the previous several frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped state consists of 4 grey-scaled consecutive frames. Each time Mario makes an action, the environment responds with a state of this structure. The structure is represented by a 3D array of the size `[4, 84, 84]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
